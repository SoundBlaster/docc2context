# F3.1 ‚Äì Performance Regression CI (Planning)

**Status:** SELECT_NEXT (planning only)  
**Date:** 2025-12-13  
**Owner:** docc2context agent  
**Depends On:** F3 Performance Benchmark Harness (complete), existing CI + release gates

---

## üéØ Intent

Design an opt-in CI job (PR label or nightly) that runs `docc2context-benchmark` against a deterministic synthetic ~10 MB fixture, compares results to a stored baseline with tolerance, and surfaces regressions without introducing flaky merge blockers.

---

## üß≠ Context & References
- PRD non-functional target: convert ~10 MB bundle ‚â§10s on modern Apple Silicon.
- F3 harness + `BenchmarkFixtureBuilder` already landed; README and fixtures docs updated.
- CI constraints: shared runners have timing variance; should avoid blocking merges by default.
- Existing gating scripts (`release_gates.sh`) focus on determinism/coverage; performance is additive.

---

## üìå Scope (Planning Only)
- Define baseline storage format (checked-in JSON with version + tolerance vs. artifact-only).
- Decide trigger model: PR label (e.g., `perf-check`), workflow_dispatch, or nightly scheduled.
- Specify regression detection rules: e.g., maxDuration > (baseline * 1.5) or > fixed threshold.
- Determine outputs: GitHub artifact of metrics JSON; optional PR comment on regression.
- Keep fixture synthetic (10 MB) to stay deterministic/fast; avoid real large bundles in CI.

---

## üîé Current State
- Harness executable: `swift run docc2context-benchmark` with `--synthesize-megabytes`, `--metrics-json`, `--iterations`, `--threshold-seconds`, `--keep-output`.
- Baseline run (local): ~0.02s max on 10 MB synthetic bundle; easily under 10s target.
- No CI job yet; no stored baseline/tolerance logic.

---

## üìù Proposed Plan for START
1) **Baseline definition:** Record a canonical metrics JSON from a controlled machine; add a comparer script (Swift or Python) that reads baseline + new metrics and reports deltas with tolerance.
2) **Workflow wiring:** Add a GitHub Actions job that builds release mode, runs the benchmark with `--synthesize-megabytes 10 --iterations 3 --threshold-seconds 10`, saves metrics, compares to baseline, and posts a summary. Trigger via label or manual dispatch.
3) **Flake mitigation:** Use generous tolerance and non-blocking status (comment/artifact) initially; consider promote-to-required only after variance is measured.
4) **Docs:** Update README/SECRETS/workflow notes on how to enable/interpret the performance check.

---

## üöß Risks & Notes
- Timing variance on shared runners could produce false regressions; must keep gate non-blocking until variance is characterized.
- Baseline drift: need a controlled process to refresh baselines when intentional perf changes land.
- Artifact retention: ensure metrics JSON is uploaded for traceability without bloating CI storage.

---

## ‚úÖ Definition of Done (for START)
- New CI workflow/job runs the harness with deterministic inputs and exports metrics.
- Regression check script compares against a stored baseline with configurable tolerance.
- Results are visible (artifact + optional PR comment) without blocking merges by default.
- Documentation updated; tests covering the comparer logic and workflow script paths.
