# F3.1 â€“ Performance Regression CI

**Status:** Completed  
**Date:** 2025-12-14  
**Owner:** docc2context agent  
**Depends On:** F3 Performance Benchmark Harness (complete), existing CI + release gates

---

## ðŸŽ¯ Intent

Design an opt-in CI job (PR label or nightly) that runs `docc2context-benchmark` against a deterministic synthetic ~10 MB fixture, compares results to a stored baseline with tolerance, and surfaces regressions without introducing flaky merge blockers.

---

## ðŸ§­ Context & References
- PRD non-functional target: convert ~10 MB bundle â‰¤10s on modern Apple Silicon.
- F3 harness + `BenchmarkFixtureBuilder` already landed; README and fixtures docs updated.
- CI constraints: shared runners have timing variance; should avoid blocking merges by default.
- Existing gating scripts (`release_gates.sh`) focus on determinism/coverage; performance is additive.

---

## ðŸ“Œ Scope
- Baseline format: checked-in JSON under `Benchmarks/performance-baseline.json` with synthetic 10 MB fixture metrics.
- Trigger model: opt-in CI workflow (`Performance Benchmark`) runs on `perf-check` PR label or manual dispatch; non-blocking by default.
- Regression rules: comparator flags when average/max exceed baseline * tolerance (defaults x2) or threshold (10s).
- Outputs: metrics JSON artifact; CLI exit non-zero on regression when `--fail-on-regression` set (used by CI job).
- Fixture choice: synthetic 10 MB bundle via `--synthesize-megabytes` to remain deterministic/fast.

---

## ðŸ”Ž Current State
- Harness executable: `swift run docc2context-benchmark` with `--synthesize-megabytes`, `--metrics-json`, `--iterations`, `--threshold-seconds`, `--keep-output`.
- Baseline run (local): ~0.02s max on 10 MB synthetic bundle; easily under 10s target.
- No CI job yet; no stored baseline/tolerance logic.

---

## âœ… Completion Notes
- Added `BenchmarkComparator` with tolerance-based comparison, baseline loader, and `BenchmarkCommand` flags (`--baseline`, `--tolerance-average`, `--tolerance-max`, `--fail-on-regression`).
- Stored deterministic baseline metrics at `Benchmarks/performance-baseline.json`.
- Added `Performance Benchmark` GitHub Actions workflow (manual or `perf-check` label) that runs `docc2context-benchmark` with synthetic 10 MB fixture, compares against baseline, and uploads metrics artifact.
- Tests added: `BenchmarkComparatorTests` plus `BenchmarkCommandBaselineTests` exercising regression exit on baseline failure; full `swift test` passes.
- README updated with baseline comparison examples and CI usage; performance fixture notes remain deterministic/offline.
